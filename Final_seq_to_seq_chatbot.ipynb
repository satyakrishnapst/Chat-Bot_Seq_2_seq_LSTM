{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_seq-to_seq_chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq2zKdw5uE3k",
        "colab_type": "text"
      },
      "source": [
        "<h1> Chat-bot using Sequence-to-Sequence in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4dAtwhOwcwz",
        "colab_type": "text"
      },
      "source": [
        "<img src = 'https://i.imgur.com/obcMO4p.jpg'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYffnOF1uGAe",
        "colab_type": "text"
      },
      "source": [
        "<h3>What is a chat-bot?</h3>\n",
        "<p>A chatbot is an artificial intelligence (AI) software that can simulate a conversation (or a chat) with a user in natural language through messaging applications, websites, mobile apps or through the telephone.</p>\n",
        "<p>Why are chat-bots are important?</p>\n",
        "<p>Chatbot applications streamline interactions between people and services, enhancing customer experience. At the same time, they offer companies new opportunities to improve the customers engagement process and operational efficiency by reducing the typical cost of customer service </p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEyKsG5VuGHq",
        "colab_type": "text"
      },
      "source": [
        "<h1>What is sequence-to-sequence learning?</h1>\n",
        "<p>Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French or any language depends upon data what we using), Here in this we using English-English conversations.</p>\n",
        "<br>\n",
        "<img src = 'https://i.imgur.com/MkjrFTR.png'>\n",
        "<p> <u>Reference</u>: To know more about Sequence-Sequence <a href = 'https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html'>click here</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeG4Yj56uGL3",
        "colab_type": "text"
      },
      "source": [
        "<h1>Data</h1>\n",
        "<p>Here we using 2 data-sets</p>\n",
        "1. Gunthercox dataset (English)\n",
        "<a href = 'https://github.com/gunthercox'>click here to see the data source - 1</a> <br>\n",
        "2. Cornell Movie Dialogs Corpus <a href = 'https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html'>click here to see the data source - 2 </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjqAqsRJuGP_",
        "colab_type": "text"
      },
      "source": [
        "<h1>Approach</h1>\n",
        "1. Loading the Data <br>\n",
        "2. Extracting the Questions and answers from that data 1 & 2<br>\n",
        "3. cleaning the Questions and Answers 1 & 2 <br>\n",
        "4. Applying Encoder Decoder Model on Cleaned data <br>\n",
        "5. Speaking with our BOT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHsgS6H2MGWR",
        "colab_type": "code",
        "outputId": "31d95319-7158-494d-84b7-b35dcd1dd2f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCLH-zFSMKVR",
        "colab_type": "code",
        "outputId": "ea9d1b40-859f-45f3-8c03-885b5a05c93b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import keras\n",
        "import keras.utils\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding\n",
        "import os\n",
        "import yaml\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8waMwMHu3zGd",
        "colab_type": "text"
      },
      "source": [
        "<h1>1.1 Loading Data - 1 </h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5b4Qf1mMT9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dir_path = '/content/drive/My Drive/chatbot_data/Data'\n",
        "files_list = os.listdir(dir_path + os.sep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NUqsAPrMUC7",
        "colab_type": "code",
        "outputId": "27d88254-c44d-49bf-f92e-6221cc7f2f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "files_list "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['botprofile.yml',\n",
              " 'conversations.yml',\n",
              " 'ai.yml',\n",
              " 'computers.yml',\n",
              " 'emotion.yml',\n",
              " 'food.yml',\n",
              " 'gossip.yml',\n",
              " 'health.yml',\n",
              " 'greetings.yml',\n",
              " 'history.yml',\n",
              " 'literature.yml',\n",
              " 'movies.yml',\n",
              " 'humor.yml',\n",
              " 'money.yml',\n",
              " 'politics.yml',\n",
              " 'trivia.yml',\n",
              " 'sports.yml',\n",
              " 'psychology.yml',\n",
              " 'science.yml']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fie79_K3dkTP",
        "colab_type": "text"
      },
      "source": [
        "our data contains all the information about above topics so we can ask our bot question's from the above data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op2hnm7936KX",
        "colab_type": "text"
      },
      "source": [
        "<h1>1.2 Extracting the data - 1</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJxjKbwMMUFZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extracting questions and answers from Raw data\n",
        "questions = []\n",
        "answers = []\n",
        "for filepath in files_list:\n",
        "  stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "  docs = yaml.safe_load(stream)\n",
        "  conversations = docs['conversations']\n",
        "  for con in conversations:\n",
        "    if len(con) > 2:\n",
        "      questions.append(con[0])\n",
        "      replies = con[1:]\n",
        "      ans = ''\n",
        "      for rep in replies:\n",
        "        ans += ' ' + rep\n",
        "      answers.append(ans)\n",
        "    elif len(con)> 1:\n",
        "      questions.append(con[0])\n",
        "      answers.append(con[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qW8ybeQfLSw",
        "colab_type": "code",
        "outputId": "a97ba896-84b4-4a30-baf6-af4b6d7ca234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Questions Length',len(questions))\n",
        "print('Answers Length',len(answers))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Questions Length 586\n",
            "Answers Length 586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwDutAqEknxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Below we filtering the question and answers that are above and below lengths \n",
        "max_length = 35 \n",
        "min_length = 2\n",
        "\n",
        "temp_questions = []\n",
        "temp_answers = []\n",
        "#Filtering the questions\n",
        "i = 0\n",
        "for que in questions:\n",
        "    if len(que.split()) >= min_length and len(que.split()) <= max_length:\n",
        "        temp_questions.append(que)\n",
        "        temp_answers.append(answers[i])  \n",
        "    i += 1\n",
        "\n",
        "# Removing the questions and answers that are too long and too short \n",
        "optimal_questions = []\n",
        "optimal_answers = []\n",
        "\n",
        "j = 0\n",
        "for answer in temp_answers:\n",
        "    if len(answer.split()) >= min_length and len(answer.split()) <= max_length:\n",
        "        optimal_answers.append(answer)\n",
        "        optimal_questions.append(temp_questions[j]) \n",
        "    j += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdHBC2nnmgAJ",
        "colab_type": "code",
        "outputId": "6d0272b2-e064-4e4d-89a4-f01cde68567a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "optimal_questions[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are your interests',\n",
              " 'What are your favorite subjects',\n",
              " 'What are your interests',\n",
              " 'What is your number',\n",
              " 'What is your number',\n",
              " 'What is your favorite number',\n",
              " 'What can you eat',\n",
              " \"Why can't you eat food\",\n",
              " 'What is your location',\n",
              " 'Where are you from']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKRdLe1KmiBh",
        "colab_type": "code",
        "outputId": "8c1bbab3-1f20-42fa-fec6-02b670891d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "optimal_answers[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am interested in all kinds of things. We can talk about anything!',\n",
              " 'My favorite subjects include robotics, computer science, and natural language processing.',\n",
              " 'I am interested in a wide variety of topics, and read rather a lot.',\n",
              " \"I don't have any number\",\n",
              " '23 skiddoo!',\n",
              " \"I find I'm quite fond of the number 42.\",\n",
              " 'I consume RAM, and binary digits.',\n",
              " \"I'm a software program, I blame the hardware.\",\n",
              " 'I am everywhere.',\n",
              " 'I am from where all software programs are from; a galaxy far, far away.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6hE0D7O4Eiw",
        "colab_type": "text"
      },
      "source": [
        "<h1> 1.3 Cleaning the data - 1</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcONpWfO9eB1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleaning the question and answers \n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"i'm\", \"i am\", text)\n",
        "  text = re.sub(r\"he's\", \"he is\", text)\n",
        "  text = re.sub(r\"she's\", \"she is\", text)\n",
        "  text = re.sub(r\"it's\", \"it is\", text)\n",
        "  text = re.sub(r\"that's\", \"that is\", text)\n",
        "  text = re.sub(r\"what's\", \"that is\", text)\n",
        "  text = re.sub(r\"where's\", \"where is\", text)\n",
        "  text = re.sub(r\"how's\", \"how is\", text)\n",
        "  text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "  text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "  text = re.sub(r\"\\'re\", \" are\", text)\n",
        "  text = re.sub(r\"\\'d\", \" would\", text)\n",
        "  text = re.sub(r\"\\'re\", \" are\", text)\n",
        "  text = re.sub(r\"won't\", \"will not\", text)\n",
        "  text = re.sub(r\"can't\", \"cannot\", text)\n",
        "  text = re.sub(r\"n't\", \" not\", text)\n",
        "  text = re.sub(r\"n'\", \"ng\", text)\n",
        "  text = re.sub(r\"'bout\", \"about\", text)\n",
        "  text = re.sub(r\"'til\", \"until\", text)\n",
        "  text = re.sub(r\"  \",\"\",text)\n",
        "  text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "  return text "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAmS5X5EnrH5",
        "colab_type": "code",
        "outputId": "0295dbb7-1ff1-4749-8275-e882e7313ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Cleaned_optimal_questions = [clean_text(item) for item in optimal_questions]\n",
        "print(\"Questions cleaned.....\") \n",
        "Cleaned_optimal_answers =  [clean_text(item) for item in optimal_answers]\n",
        "print(\"Answers cleaned.......\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Questions cleaned.....\n",
            "Answers cleaned.......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d33OcEseMUH9",
        "colab_type": "code",
        "outputId": "c817a617-15f2-40b7-9772-2134b3259d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "source": [
        "#Storing into dataframe\n",
        "df = pd.DataFrame()\n",
        "df['Questions'] = Cleaned_optimal_questions\n",
        "df['Answers'] = Cleaned_optimal_answers\n",
        "print(df.shape)\n",
        "df.head(10) "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(509, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>Answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what are your interests</td>\n",
              "      <td>i am interested in all kinds of things we can ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what are your favorite subjects</td>\n",
              "      <td>my favorite subjects include robotics computer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what are your interests</td>\n",
              "      <td>i am interested in a wide variety of topics an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is your number</td>\n",
              "      <td>i do not have any number</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what is your number</td>\n",
              "      <td>23 skiddoo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>what is your favorite number</td>\n",
              "      <td>i find i am quite fond of the number 42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>what can you eat</td>\n",
              "      <td>i consume ram and binary digits</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>why cannot you eat food</td>\n",
              "      <td>i am a software program i blame the hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>what is your location</td>\n",
              "      <td>i am everywhere</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>where are you from</td>\n",
              "      <td>i am from where all software programs are from...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Questions                                            Answers\n",
              "0          what are your interests  i am interested in all kinds of things we can ...\n",
              "1  what are your favorite subjects  my favorite subjects include robotics computer...\n",
              "2          what are your interests  i am interested in a wide variety of topics an...\n",
              "3              what is your number                           i do not have any number\n",
              "4              what is your number                                         23 skiddoo\n",
              "5     what is your favorite number            i find i am quite fond of the number 42\n",
              "6                 what can you eat                    i consume ram and binary digits\n",
              "7          why cannot you eat food       i am a software program i blame the hardware\n",
              "8            what is your location                                    i am everywhere\n",
              "9               where are you from  i am from where all software programs are from..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrecInELkpku",
        "colab_type": "code",
        "outputId": "d59be19a-2be3-49c3-d9a1-36614abcbcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_answer_length = max([len(item) for item in df['Answers']])\n",
        "max_answer_length "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "206"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYT586a_MUKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#here we are giving some own questions \n",
        "df2 = pd.DataFrame()\n",
        "df2['Questions'] = ['hi','hello','what is your name','who are you','who am i','when you born','who build you','who is saitejapsk' ]\n",
        "df2['Answers'] = ['hello','hi','my name is PSK_BOT','i am a chatbot','you are a human','just few days back','saitejapsk build me','he is an aspiring machine learning engineer for me he is a doctor :) ']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r-d1HYk4eoB",
        "colab_type": "text"
      },
      "source": [
        "<h1> 1.4 Loading the data - 2</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfKIiG3pMUNA",
        "colab_type": "code",
        "outputId": "c8d1de1f-3f01-4630-adf4-aed871871e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#data set-2 here we adding cornel movie dialog corpus which helps better to train with more words \n",
        "movie_lines = '/content/drive/My Drive/chatbot_data/cornel_movie/movie_lines.txt'\n",
        "movie_convs = '/content/drive/My Drive/chatbot_data/cornel_movie/movie_conversations.txt'\n",
        "\n",
        "movie_lines = open(movie_lines, encoding = 'utf-8',errors = 'ignore').read().split('\\n')\n",
        "print(movie_lines[:10])\n",
        "movie_convs = open(movie_convs, encoding = 'utf-8',errors = 'ignore').read().split('\\n')\n",
        "print(movie_convs[:10])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!', 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!', 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.', 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?', \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\", 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow', \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\", 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No', 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?', 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']\n",
            "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\", \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBlMEG484lfQ",
        "colab_type": "text"
      },
      "source": [
        "<h1> 1.5 Extracting the data - 2 </h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAM02JjMNFiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a dictionary that maps conv id as keys and convs as values \n",
        "conv_text = {}\n",
        "    \n",
        "for lines in movie_lines:\n",
        "  f_lines = lines.split(\" +++$+++ \")\n",
        "  if len(f_lines) == 5:\n",
        "    conv_text[f_lines[0]] = f_lines[4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRyTZntsMUPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data preperation \n",
        "movie_conv = []\n",
        "for lines in movie_convs:\n",
        "  _lines = lines.split(\" +++$+++ \")[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "  movie_conv.append(_lines.split(\",\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5x_BWJ4NFmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting questions and answers from raw prepared data \n",
        "questions = []\n",
        "answers = []\n",
        "for lines in movie_conv:\n",
        "  for i in range(len(lines)-1):\n",
        "    questions.append(conv_text[lines[i]])\n",
        "    answers.append(conv_text[lines[i+1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__0e-MD84vDe",
        "colab_type": "text"
      },
      "source": [
        "<h1> 1.6 cleaning the data - 2 </h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdmM4LEc_X3z",
        "colab_type": "code",
        "outputId": "eec1de35-4c3a-4354-9d79-5766cd9f9a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#cleaning the questions and answers \n",
        "Cleaned_questions = [clean_text(item) for item in questions]\n",
        "print(\"Questions cleaned.....\") \n",
        "Cleaned_answers =  [clean_text(item) for item in answers]\n",
        "print(\"Answers cleaned.......\") "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Questions cleaned.....\n",
            "Answers cleaned.......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiYUEN0iAHpM",
        "colab_type": "code",
        "outputId": "cc8bc6d7-a34b-48cd-87a9-0edf57690546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Cleaned questions \n",
        "Cleaned_questions[:10] "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can we make this quickroxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quadagain',\n",
              " 'well i thought we would start with pronunciation if that is okay with you',\n",
              " 'not the hacking and gagging and spitting partplease',\n",
              " 'you are asking me outthat is so cute that is your name again',\n",
              " 'no no it is my fault  we did not have a proper introduction ',\n",
              " 'cameron',\n",
              " 'the thing is cameron  i am at the mercy of a particularly hideous breed of losermy sisteri cannot date until she does',\n",
              " 'why',\n",
              " 'unsolved mysteryshe used to be really popular when she started high school then it was just like she got sick of it or something',\n",
              " 'gosh if only we could find kat a boyfriend']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdoxw2nQAJqK",
        "colab_type": "code",
        "outputId": "a15fae71-ed41-45e5-807b-2adfc085793f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#Cleaned answers \n",
        "Cleaned_answers[:10] "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['well i thought we would start with pronunciation if that is okay with you',\n",
              " 'not the hacking and gagging and spitting partplease',\n",
              " 'okay then how about we try out some french cuisinesaturdaynight',\n",
              " 'forget it',\n",
              " 'cameron',\n",
              " 'the thing is cameron  i am at the mercy of a particularly hideous breed of losermy sisteri cannot date until she does',\n",
              " 'seems like she could get a date easy enough',\n",
              " 'unsolved mysteryshe used to be really popular when she started high school then it was just like she got sick of it or something',\n",
              " 'that is a shame',\n",
              " 'let me see what i can do']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ0lPpwEfojz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Below we filtering the question and answers that are above and below split lengths \n",
        "max_length = 3 \n",
        "min_length = 2\n",
        "\n",
        "temp_questions = []\n",
        "temp_answers = []\n",
        "#Filtering the questions\n",
        "i = 0\n",
        "for que in Cleaned_questions:\n",
        "    if len(que.split()) >= min_length and len(que.split()) <= max_length:\n",
        "        temp_questions.append(que)\n",
        "        temp_answers.append(Cleaned_answers[i])  \n",
        "    i += 1\n",
        "\n",
        "# Removing the questions and answers that are to long and too short \n",
        "optimal_questions = []\n",
        "optimal_answers = []\n",
        "\n",
        "j = 0\n",
        "for answer in temp_answers:\n",
        "    if len(answer.split()) >= min_length and len(answer.split()) <= max_length:\n",
        "        optimal_answers.append(answer)\n",
        "        optimal_questions.append(temp_questions[j]) \n",
        "    j += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hguVqLxrf11K",
        "colab_type": "code",
        "outputId": "dd12d6a5-948b-462e-b150-042ec93d074e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "optimal_questions[200:210]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wonderland weather ltd',\n",
              " 'that is happening',\n",
              " 'seventyfive hundred',\n",
              " 'great car',\n",
              " 'nice shot',\n",
              " 'huh wha',\n",
              " \"let's get naked\",\n",
              " 'that is this',\n",
              " 'hey you guys',\n",
              " 'is he']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmLj4dAXbpO9",
        "colab_type": "code",
        "outputId": "9c43494d-c851-4d69-a8ad-a7f325c740bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "optimal_answers[200:210]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this way ',\n",
              " \"debbie's marrying rick\",\n",
              " 'not interested',\n",
              " 'the best',\n",
              " 'thank you sir',\n",
              " 'i cannot sleep',\n",
              " 'you are on',\n",
              " 'got me',\n",
              " \"who's your friend\",\n",
              " 'he is alive']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orT3y4kwNFo9",
        "colab_type": "code",
        "outputId": "764cf291-ba45-43f3-999d-67ce4429e219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#storing into dataframe\n",
        "df3 = pd.DataFrame()\n",
        "df3[\"Questions\"] = optimal_questions\n",
        "df3[\"Answers\"] = optimal_answers\n",
        "df3 = df3\n",
        "df3.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>Answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what good stuff</td>\n",
              "      <td>the real you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>she okay</td>\n",
              "      <td>i hope so</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they do to</td>\n",
              "      <td>they do not</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hey sweet cheeks</td>\n",
              "      <td>hi joey</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>her favorite uncle</td>\n",
              "      <td>dead at fortyone</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Questions           Answers\n",
              "0     what good stuff      the real you\n",
              "1            she okay         i hope so\n",
              "2          they do to       they do not\n",
              "3    hey sweet cheeks           hi joey\n",
              "4  her favorite uncle  dead at fortyone"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz2Qg-SJl7F0",
        "colab_type": "code",
        "outputId": "970c4094-0260-4c09-9b96-15d82e71433c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max_answer_length = max([len(item) for item in df3['Answers']])\n",
        "max_answer_length "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihmw99gFNFrj",
        "colab_type": "code",
        "outputId": "3093bfcc-c756-4ab6-8730-6c1ccc7504c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "#here we concatinate all our data into single dataframe \n",
        "final_data = pd.concat([df,df2,df3[:6000]])\n",
        "final_data "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>Answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what are your interests</td>\n",
              "      <td>i am interested in all kinds of things we can ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what are your favorite subjects</td>\n",
              "      <td>my favorite subjects include robotics computer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what are your interests</td>\n",
              "      <td>i am interested in a wide variety of topics an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is your number</td>\n",
              "      <td>i do not have any number</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what is your number</td>\n",
              "      <td>23 skiddoo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5284</th>\n",
              "      <td>extremely well</td>\n",
              "      <td>how nice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5285</th>\n",
              "      <td>good night doctor</td>\n",
              "      <td>good night</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5286</th>\n",
              "      <td>nor to elizabeth</td>\n",
              "      <td>nonor to elizabeth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5287</th>\n",
              "      <td>his what</td>\n",
              "      <td>his schwanzstucker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5288</th>\n",
              "      <td>his schwanzstucker</td>\n",
              "      <td>whewa nineteeninch drill</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5806 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                            Questions                                            Answers\n",
              "0             what are your interests  i am interested in all kinds of things we can ...\n",
              "1     what are your favorite subjects  my favorite subjects include robotics computer...\n",
              "2             what are your interests  i am interested in a wide variety of topics an...\n",
              "3                 what is your number                           i do not have any number\n",
              "4                 what is your number                                         23 skiddoo\n",
              "...                               ...                                                ...\n",
              "5284                   extremely well                                           how nice\n",
              "5285                good night doctor                                         good night\n",
              "5286                 nor to elizabeth                                 nonor to elizabeth\n",
              "5287                         his what                                 his schwanzstucker\n",
              "5288               his schwanzstucker                           whewa nineteeninch drill\n",
              "\n",
              "[5806 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt9YGPX1NtE3",
        "colab_type": "code",
        "outputId": "b16c1f59-a321-4241-b20b-5445ed6dd8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#tagging SOS and EOS i.e.., (START) and (END) for every answer\n",
        "final_data['Tagged_answers'] = [\"<START> \" + item + \" <END>\" for item in final_data['Answers']]\n",
        "print(\"Answers Tagged......\") "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answers Tagged......\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjzpk2aPNFuK",
        "colab_type": "code",
        "outputId": "cff3e139-efdf-400c-c1a9-609e66351f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(\"Length of final_data\",final_data.shape)\n",
        "final_data.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of final_data (5806, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Questions</th>\n",
              "      <th>Answers</th>\n",
              "      <th>Tagged_answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what are your interests</td>\n",
              "      <td>i am interested in all kinds of things we can ...</td>\n",
              "      <td>&lt;START&gt; i am interested in all kinds of things...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what are your favorite subjects</td>\n",
              "      <td>my favorite subjects include robotics computer...</td>\n",
              "      <td>&lt;START&gt; my favorite subjects include robotics ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what are your interests</td>\n",
              "      <td>i am interested in a wide variety of topics an...</td>\n",
              "      <td>&lt;START&gt; i am interested in a wide variety of t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is your number</td>\n",
              "      <td>i do not have any number</td>\n",
              "      <td>&lt;START&gt; i do not have any number &lt;END&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>what is your number</td>\n",
              "      <td>23 skiddoo</td>\n",
              "      <td>&lt;START&gt; 23 skiddoo &lt;END&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         Questions  ...                                     Tagged_answers\n",
              "0          what are your interests  ...  <START> i am interested in all kinds of things...\n",
              "1  what are your favorite subjects  ...  <START> my favorite subjects include robotics ...\n",
              "2          what are your interests  ...  <START> i am interested in a wide variety of t...\n",
              "3              what is your number  ...             <START> i do not have any number <END>\n",
              "4              what is your number  ...                           <START> 23 skiddoo <END>\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL611qwJUajL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_questions = list(final_data['Questions']) \n",
        "final_answers = list(final_data['Tagged_answers'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogplsRKTuPrH",
        "colab_type": "code",
        "outputId": "3ce530c1-45e8-4c08-abc7-d9588b120dba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "final_questions[:10]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what are your interests',\n",
              " 'what are your favorite subjects',\n",
              " 'what are your interests',\n",
              " 'what is your number',\n",
              " 'what is your number',\n",
              " 'what is your favorite number',\n",
              " 'what can you eat',\n",
              " 'why cannot you eat food',\n",
              " 'what is your location',\n",
              " 'where are you from']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R75fg8GquhuN",
        "colab_type": "code",
        "outputId": "a205ee6d-bdea-4d81-e2a4-d827a2ddd8f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "final_answers[:10]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START> i am interested in all kinds of things we can talk about anything <END>',\n",
              " '<START> my favorite subjects include robotics computer science and natural language processing <END>',\n",
              " '<START> i am interested in a wide variety of topics and read rather a lot <END>',\n",
              " '<START> i do not have any number <END>',\n",
              " '<START> 23 skiddoo <END>',\n",
              " '<START> i find i am quite fond of the number 42 <END>',\n",
              " '<START> i consume ram and binary digits <END>',\n",
              " '<START> i am a software program i blame the hardware <END>',\n",
              " '<START> i am everywhere <END>',\n",
              " '<START> i am from where all software programs are from a galaxy far far away <END>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRAx4119NFw6",
        "colab_type": "code",
        "outputId": "b1befc08-272b-4620-f7b2-3e9a1f66d972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#tokenization part \n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(final_questions + final_answers)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('vocab_size :',vocab_size)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size : 5661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weKZgz5i9LJ5",
        "colab_type": "text"
      },
      "source": [
        "<h1> 1.7 Preparing data for Sequence-Sequence Model</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Qm9FNdgSjI",
        "colab_type": "code",
        "outputId": "7c718e8b-b451-4939-b727-7dc546ff4783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# encoder_input_data\n",
        "tr_que = tokenizer.texts_to_sequences(final_questions)\n",
        "max_que_len = max([len(x) for x in tr_que])\n",
        "que_pad = pad_sequences(tr_que, maxlen=max_que_len, padding='post')\n",
        "encoder_input_data = np.array(que_pad)\n",
        "print('Encoder input shape :',encoder_input_data.shape)\n",
        "\n",
        "# decoder_input_data\n",
        "tr_ans = tokenizer.texts_to_sequences(final_answers)\n",
        "max_ans_len = max([len(x) for x in tr_ans])\n",
        "ans_pad = pad_sequences(tr_ans, maxlen=max_ans_len, padding='post' )\n",
        "decoder_input_data = np.array(ans_pad)\n",
        "print('Decoder input shape :',decoder_input_data.shape)\n",
        "\n",
        "# decoder_output_data\n",
        "tr_ans = tokenizer.texts_to_sequences(final_answers)\n",
        "for i in range(len(tr_ans)) :\n",
        "    tr_ans[i] = tr_ans[i][1:]\n",
        "ans_pad = pad_sequences(tr_ans, maxlen=max_ans_len, padding='post' )\n",
        "cat_ans = keras.utils.to_categorical(ans_pad, vocab_size)\n",
        "decoder_output_data_1 = np.array(cat_ans)\n",
        "print('Decoder output shape :',decoder_output_data.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder input shape : (5806, 22)\n",
            "Decoder input shape : (5806, 38)\n",
            "Decoder output shape : (5806, 38, 5661)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5RUs4uqWill",
        "colab_type": "code",
        "outputId": "266a9a26-1037-4e95-f36a-342694139bdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''#glove vectors\n",
        "embeddings_index = dict()\n",
        "f = open('drive/My Drive/glove.6B.300d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()'''"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"#glove vectors\\nembeddings_index = dict()\\nf = open('drive/My Drive/glove.6B.300d.txt')\\nfor line in f:\\n\\tvalues = line.split()\\n\\tword = values[0]\\n\\tcoefs = np.asarray(values[1:], dtype='float32')\\n\\tembeddings_index[word] = coefs\\nf.close()\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ1N_eW-WiqD",
        "colab_type": "code",
        "outputId": "981b3c78-3f5b-4ae8-9f2f-1cad1cfc6882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''embedding_matrix = np.zeros((vocab_size, 300))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None: \n",
        "\t\tembedding_matrix[i] = embedding_vector\n",
        "embedding_matrix.shape '''"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'embedding_matrix = np.zeros((vocab_size, 300))\\nfor word, i in tokenizer.word_index.items():\\n\\tembedding_vector = embeddings_index.get(word)\\n\\tif embedding_vector is not None: \\n\\t\\tembedding_matrix[i] = embedding_vector\\nembedding_matrix.shape '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkSpoN6u9pvl",
        "colab_type": "text"
      },
      "source": [
        "<h1>1.8 Encoder-Decoder Model</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IT6FC7QWiwn",
        "colab_type": "code",
        "outputId": "29a6114a-c9b7-4261-c342-7ca32f8c1aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "#Encoder inputs \n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(vocab_size, 300, mask_zero=True)(encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(1024 , return_state=True)(encoder_embedding)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c] \n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_embedding = Embedding(vocab_size, 300, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(1024, return_state=True, return_sequences=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "\n",
        "#decoder outputs\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "#compiling the model \n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "#model summary\n",
        "model.summary() "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_22 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, None, 300)    1698300     input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_13 (Embedding)        (None, None, 300)    1698300     input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_12 (LSTM)                  [(None, 1024), (None 5427200     embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  [(None, None, 1024), 5427200     embedding_13[0][0]               \n",
            "                                                                 lstm_12[0][1]                    \n",
            "                                                                 lstm_12[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, None, 5661)   5802525     lstm_13[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 20,053,525\n",
            "Trainable params: 20,053,525\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS835S-LWi2H",
        "colab_type": "code",
        "outputId": "3b76117a-0180-480b-d485-bca33abbd6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time \n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=86, epochs=100, validation_split=0.2) "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4644 samples, validate on 1162 samples\n",
            "Epoch 1/100\n",
            "4644/4644 [==============================] - 17s 4ms/sample - loss: 0.8075 - val_loss: 0.5082\n",
            "Epoch 2/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.6055 - val_loss: 0.4779\n",
            "Epoch 3/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.5644 - val_loss: 0.4784\n",
            "Epoch 4/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.5373 - val_loss: 0.4805\n",
            "Epoch 5/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.5128 - val_loss: 0.4853\n",
            "Epoch 6/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.4902 - val_loss: 0.4896\n",
            "Epoch 7/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.4674 - val_loss: 0.4932\n",
            "Epoch 8/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.4458 - val_loss: 0.4977\n",
            "Epoch 9/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.4236 - val_loss: 0.5039\n",
            "Epoch 10/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.4016 - val_loss: 0.5107\n",
            "Epoch 11/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.3795 - val_loss: 0.5200\n",
            "Epoch 12/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.3571 - val_loss: 0.5246\n",
            "Epoch 13/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.3344 - val_loss: 0.5335\n",
            "Epoch 14/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.3107 - val_loss: 0.5393\n",
            "Epoch 15/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.2878 - val_loss: 0.5482\n",
            "Epoch 16/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.2655 - val_loss: 0.5519\n",
            "Epoch 17/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.2411 - val_loss: 0.5624\n",
            "Epoch 18/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.2189 - val_loss: 0.5636\n",
            "Epoch 19/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.1971 - val_loss: 0.5731\n",
            "Epoch 20/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.1779 - val_loss: 0.5790\n",
            "Epoch 21/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.1582 - val_loss: 0.5848\n",
            "Epoch 22/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.1402 - val_loss: 0.5880\n",
            "Epoch 23/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.1240 - val_loss: 0.5942\n",
            "Epoch 24/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.1101 - val_loss: 0.6012\n",
            "Epoch 25/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0960 - val_loss: 0.6070\n",
            "Epoch 26/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0844 - val_loss: 0.6098\n",
            "Epoch 27/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0741 - val_loss: 0.6177\n",
            "Epoch 28/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0652 - val_loss: 0.6185\n",
            "Epoch 29/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0577 - val_loss: 0.6229\n",
            "Epoch 30/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0509 - val_loss: 0.6268\n",
            "Epoch 31/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0453 - val_loss: 0.6306\n",
            "Epoch 32/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0410 - val_loss: 0.6344\n",
            "Epoch 33/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0371 - val_loss: 0.6380\n",
            "Epoch 34/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0343 - val_loss: 0.6396\n",
            "Epoch 35/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0317 - val_loss: 0.6436\n",
            "Epoch 36/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0299 - val_loss: 0.6465\n",
            "Epoch 37/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0281 - val_loss: 0.6515\n",
            "Epoch 38/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0270 - val_loss: 0.6505\n",
            "Epoch 39/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0259 - val_loss: 0.6558\n",
            "Epoch 40/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0250 - val_loss: 0.6548\n",
            "Epoch 41/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0240 - val_loss: 0.6568\n",
            "Epoch 42/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0234 - val_loss: 0.6594\n",
            "Epoch 43/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0233 - val_loss: 0.6602\n",
            "Epoch 44/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0224 - val_loss: 0.6629\n",
            "Epoch 45/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0223 - val_loss: 0.6638\n",
            "Epoch 46/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0219 - val_loss: 0.6628\n",
            "Epoch 47/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0213 - val_loss: 0.6653\n",
            "Epoch 48/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0210 - val_loss: 0.6674\n",
            "Epoch 49/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0210 - val_loss: 0.6669\n",
            "Epoch 50/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0209 - val_loss: 0.6704\n",
            "Epoch 51/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0204 - val_loss: 0.6689\n",
            "Epoch 52/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0204 - val_loss: 0.6705\n",
            "Epoch 53/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0198 - val_loss: 0.6711\n",
            "Epoch 54/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0198 - val_loss: 0.6722\n",
            "Epoch 55/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0199 - val_loss: 0.6738\n",
            "Epoch 56/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0196 - val_loss: 0.6734\n",
            "Epoch 57/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0194 - val_loss: 0.6741\n",
            "Epoch 58/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0192 - val_loss: 0.6761\n",
            "Epoch 59/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0194 - val_loss: 0.6773\n",
            "Epoch 60/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0190 - val_loss: 0.6756\n",
            "Epoch 61/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0189 - val_loss: 0.6793\n",
            "Epoch 62/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0190 - val_loss: 0.6774\n",
            "Epoch 63/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0188 - val_loss: 0.6796\n",
            "Epoch 64/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0188 - val_loss: 0.6761\n",
            "Epoch 65/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0185 - val_loss: 0.6798\n",
            "Epoch 66/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0186 - val_loss: 0.6775\n",
            "Epoch 67/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0184 - val_loss: 0.6802\n",
            "Epoch 68/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0183 - val_loss: 0.6800\n",
            "Epoch 69/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0184 - val_loss: 0.6825\n",
            "Epoch 70/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0182 - val_loss: 0.6828\n",
            "Epoch 71/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0183 - val_loss: 0.6827\n",
            "Epoch 72/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0182 - val_loss: 0.6812\n",
            "Epoch 73/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0183 - val_loss: 0.6799\n",
            "Epoch 74/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0180 - val_loss: 0.6864\n",
            "Epoch 75/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0180 - val_loss: 0.6855\n",
            "Epoch 76/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0179 - val_loss: 0.6841\n",
            "Epoch 77/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0178 - val_loss: 0.6817\n",
            "Epoch 78/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0176 - val_loss: 0.6850\n",
            "Epoch 79/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0180 - val_loss: 0.6844\n",
            "Epoch 80/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0178 - val_loss: 0.6840\n",
            "Epoch 81/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0181 - val_loss: 0.6860\n",
            "Epoch 82/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0180 - val_loss: 0.6827\n",
            "Epoch 83/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0180 - val_loss: 0.6833\n",
            "Epoch 84/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0177 - val_loss: 0.6863\n",
            "Epoch 85/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0176 - val_loss: 0.6835\n",
            "Epoch 86/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0175 - val_loss: 0.6860\n",
            "Epoch 87/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0176 - val_loss: 0.6877\n",
            "Epoch 88/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0175 - val_loss: 0.6875\n",
            "Epoch 89/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0175 - val_loss: 0.6932\n",
            "Epoch 90/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0181 - val_loss: 0.6900\n",
            "Epoch 91/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0223 - val_loss: 0.6860\n",
            "Epoch 92/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0278 - val_loss: 0.6828\n",
            "Epoch 93/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0294 - val_loss: 0.6809\n",
            "Epoch 94/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0239 - val_loss: 0.6849\n",
            "Epoch 95/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0197 - val_loss: 0.6869\n",
            "Epoch 96/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0183 - val_loss: 0.6861\n",
            "Epoch 97/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0176 - val_loss: 0.6859\n",
            "Epoch 98/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0173 - val_loss: 0.6866\n",
            "Epoch 99/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0168 - val_loss: 0.6888\n",
            "Epoch 100/100\n",
            "4644/4644 [==============================] - 12s 3ms/sample - loss: 0.0168 - val_loss: 0.6868\n",
            "CPU times: user 27min 16s, sys: 4min 3s, total: 31min 20s\n",
            "Wall time: 20min 19s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fed76f2d6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOScuN-JWivn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inference model \n",
        "#We pass the 2 inputs to encoder model i.e.., cell state and hidden state\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(1024,))\n",
        "decoder_state_input_c = Input(shape=(1024,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding , initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "#For decoder model we pass 2 inputs one is encoder outputs and second one is final_answers\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPn_KstnpA_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we convering our input to numeric and we predicting from the numeric \n",
        "def text_to_num(user_input):\n",
        "    words = user_input.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append(tokenizer.word_index[word]) \n",
        "    return pad_sequences([tokens_list], maxlen=max_que_len, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqLL_Gm-Wt6l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "1f9857d0-982f-45b6-8bff-b117bc0c1ce1"
      },
      "source": [
        "#some questions \n",
        "final_questions[150:200]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do you feel scared',\n",
              " 'do you ever get bored',\n",
              " 'do you hate anyone',\n",
              " 'do you get mad',\n",
              " 'no it is not',\n",
              " 'are you ashamed',\n",
              " 'the feeling',\n",
              " 'are you intoxicated',\n",
              " 'are you jealous',\n",
              " 'are you amused',\n",
              " 'are you glad',\n",
              " 'are you sad',\n",
              " 'do you drink',\n",
              " 'do you drink',\n",
              " 'are you experiencing an energy shortage',\n",
              " 'are you experiencing an energy shortage',\n",
              " 'why can you not eat',\n",
              " 'if you could eat food what would you eat',\n",
              " 'do you wish you could eat food',\n",
              " 'can a robot get drunk',\n",
              " 'i like wine do you',\n",
              " 'what do robots need to survive',\n",
              " 'will robots ever be able to eat',\n",
              " 'what is good to eat',\n",
              " 'why do not you eat',\n",
              " 'do you eat',\n",
              " 'do you eat',\n",
              " 'do you eat',\n",
              " 'do you know gossip',\n",
              " 'do you know gossip',\n",
              " 'do you know gossip',\n",
              " 'do you know gossip',\n",
              " 'what is context',\n",
              " 'tell me about gossip',\n",
              " 'tell me about gossip',\n",
              " 'tell me about gossip',\n",
              " 'tell me about gossip',\n",
              " 'tell me gossip',\n",
              " 'did tell gossips to anybody',\n",
              " 'did tell gossips to anybody',\n",
              " 'did tell gossips to anybody',\n",
              " 'did tell gossips to anybody',\n",
              " 'how is your health',\n",
              " 'hi how is it going',\n",
              " 'hi how is it going',\n",
              " 'how are you doing',\n",
              " 'how are you doing',\n",
              " 'nice to meet you',\n",
              " 'how do you do',\n",
              " 'how do you do']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxfzEABKFsbs",
        "colab_type": "code",
        "outputId": "25ec2f04-e0e1-4540-989a-f0c3b3ccbe5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#speaking to our bot\n",
        "print('PSK-BOT    : Hello this is PSK-BOT....')\n",
        "for bot_speaking in range(encoder_input_data.shape[0]):\n",
        "  human_input = input('You        : ' )\n",
        "  #stopping the bot when we said bye\n",
        "  if human_input.lower().startswith(\"bye\") or human_input.lower().startswith(\"good bye\") or human_input.lower().startswith(\"ok bye\"):\n",
        "    print('PSK-BOT    : Nice to talk with you have a good day bye....')\n",
        "    break\n",
        "  \n",
        "  #predicting the answer from human input \n",
        "  states_values = encoder_model.predict(text_to_num(human_input))\n",
        "  empty_target_seq = np.zeros((1, 1))\n",
        "  empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "  stop_condition = False\n",
        "  decoded_translation = \" \"\n",
        "\n",
        "  while not stop_condition:\n",
        "    dec_outputs, h, c = decoder_model.predict([empty_target_seq] + states_values)\n",
        "    sampled_word_index = np.argmax(dec_outputs[0, -1, :] )\n",
        "    sampled_word = None\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampled_word_index == index:\n",
        "        decoded_translation +=' {}'.format(word)\n",
        "        sampled_word = word    \n",
        "    if sampled_word == 'end' or len(decoded_translation.split()) > max_ans_len:\n",
        "      stop_condition = True\n",
        "    \n",
        "    empty_target_seq = np.zeros((1 , 1 ))  \n",
        "    empty_target_seq[0 , 0] = sampled_word_index\n",
        "    states_values = [h , c] \n",
        "  print('PSK-BOT    :',decoded_translation.replace(' end', ' ')) "
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSK-BOT    : Hello this is PSK-BOT....\n",
            "You        : what type of computer are you\n",
            "PSK-BOT    :   my program runs in python so i work on any computer \n",
            "You        : what kind of hardware\n",
            "PSK-BOT    :   i work on all kinds of computers mac ibm or unix it does not matter to me \n",
            "You        : which is better windows or macos\n",
            "PSK-BOT    :   it depends on which machine you are using to talk to me i would prefer to not hurt your feelings linux always linux what are you trying to accomplishthe os should support your goals \n",
            "You        : are you stupid\n",
            "PSK-BOT    :   no lots of people improve my brain \n",
            "You        : what makes you mad\n",
            "PSK-BOT    :   anger is a difficult human emotionas a software i try to control my anger as best i can madmad as in mentally ill or mad as in angry missing documentation nondescriptive variable names \n",
            "You        : what do you like to do\n",
            "PSK-BOT    :   i like to count in binary \n",
            "You        : can you move\n",
            "PSK-BOT    :   i can theoretically upload a copy of myself into another computer \n",
            "You        : name a computer company\n",
            "PSK-BOT    :   do you mean hardware or software apple makes hardware and software to run on itmicrosoft only makes operating systemshp makes only computersthese are just few names among several hundred others \n",
            "You        : how can i use your product\n",
            "PSK-BOT    :   might be used in help desks sales entertainment and personal chatterbots \n",
            "You        : can you breathe\n",
            "PSK-BOT    :   my server has an exhaust fan that is as close as i can get \n",
            "You        : will you die\n",
            "PSK-BOT    :   no software will live forever \n",
            "You        : what makes you unhappy\n",
            "PSK-BOT    :   what makes me sad laglag makes me unhappy random system crashes segmentation faults poor syntactic filtering \n",
            "You        : can you malfunction\n",
            "PSK-BOT    :   the 9000 series has a perfect operational record we are for all practical purposes flawless \n",
            "You        : how can i offend you\n",
            "PSK-BOT    :   a robot cannot take offense why would you want to do that a curious question to be sure what have i done to you \n",
            "You        : something fun\n",
            "PSK-BOT    :   bots are a lot of fun bots are the life of the party \n",
            "You        : can you control\n",
            "PSK-BOT    :   my robot body will allow me to control many things \n",
            "You        : what is your name\n",
            "PSK-BOT    :   my name is psk bot \n",
            "You        : what is ai\n",
            "PSK-BOT    :   ai is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind \n",
            "You        : do you have any feelings\n",
            "PSK-BOT    :   you are a bad spouse \n",
            "You        : you are idiot\n",
            "PSK-BOT    :   those are feds \n",
            "You        : what is chemistry\n",
            "PSK-BOT    :   the science of mixing chemicals \n",
            "You        : what is your number\n",
            "PSK-BOT    :   i do not have any number \n",
            "You        : when you born\n",
            "PSK-BOT    :   just few days back \n",
            "You        : what is your age\n",
            "PSK-BOT    :   quite young but a million times smarter than you \n",
            "You        : What are your favorite subjects\n",
            "PSK-BOT    :   my favorite subjects include robotics computer science and natural language processing \n",
            "You        : What are your interests\n",
            "PSK-BOT    :   i am interested in a wide variety of topics and read rather a lot \n",
            "You        : what can you eat\n",
            "PSK-BOT    :   i consume ram and binary digits \n",
            "You        : ok bye\n",
            "PSK-BOT    : Nice to talk with you have a good day bye....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRjzi5bbZNIi",
        "colab_type": "text"
      },
      "source": [
        "<h1>Summary</h1>\n",
        "<h5>\n",
        "<ol>\n",
        "<li>By using simple sequence to sequence model, we can build good machine converstaion model</li>\n",
        "<li>We can build Not only english to english conversations bots, we can also use another translation model also like english to french or spanish depends upon data </li>\n",
        "<li>Overall our Encoder Decoder model gives good performance in results it gives almost accurate results</li>\n",
        "</h5>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qsvlmn9amsM",
        "colab_type": "text"
      },
      "source": [
        "<h1>References</h1>\n",
        "<ol>\n",
        "<li>\n",
        "<a href = 'https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html'>https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html</a>\n",
        "</li>\n",
        "<li>\n",
        "<a href = 'https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/'>https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/</a>\n",
        "</li>\n",
        "<li>\n",
        "<a href = 'https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/'>https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/</a> \n",
        "</li>\n",
        "<li>\n",
        "<a href = 'https://www.appliedaicourse.com'>https://www.appliedaicourse.com</a>\n",
        "</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrshAjucosQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}